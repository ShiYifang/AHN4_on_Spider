{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import Client, SSHCluster\n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline AHN4 Workflow - Feature Extraction (Non-Ground Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    "Choose whether you want to run all input files or run the only input files listed in `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = pathlib.Path('/project/lidarac/Software/Yifang/JupyterDaskOnSLURM/AHN4_test/')\n",
    "\n",
    "# path to normalized files \n",
    "path_input = path_root / 'Normalized'\n",
    "\n",
    "# path to targets\n",
    "path_output = path_input.parent / 'Targets_veg'\n",
    "\n",
    "run = 'all'  # 'all', 'from_file'\n",
    "#filename = 'feature_extraction_non-ground_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'from_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 4 tiles\n",
      "Retrieve and extract features for: 4 tiles\n"
     ]
    }
   ],
   "source": [
    "tiles = [el for el in path_input.iterdir() if el.match('tile_*_*.laz')]\n",
    "print('Found: {} tiles'.format(len(tiles)))\n",
    "if run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        tiles_read = json.load(f)\n",
    "    tiles_read = [path_input/f for f in tiles_read]\n",
    "    # check whether all files are available on dCache\n",
    "    assert all([f in tiles for f in tiles_read]), f'Some of the tiles in {filename} are not in input dir'\n",
    "    tiles = tiles_read\n",
    "print('Retrieve and extract features for: {} tiles'.format(len(tiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_tmp = pathlib.Path('/data/local/tmp')\n",
    "\n",
    "nprocs_per_node = 2  \n",
    "\n",
    "# start the cluster\n",
    "scheduler_node = 'node1'\n",
    "hosts = [f'node{i}' for i in range(1, 11)]\n",
    "cluster = SSHCluster(hosts=[scheduler_node] + hosts, \n",
    "                     connect_options={'known_hosts': None, \n",
    "                                      'username': 'ubuntu', \n",
    "                                      'client_keys': '/home/ubuntu/.ssh/id_rsa'},\n",
    "                     worker_options={'nthreads': 1, \n",
    "                                     'nprocs': nprocs_per_node,\n",
    "                                     'memory_limit': 0,\n",
    "                                     'local_directory': local_tmp/'dask-worker-space'}, \n",
    "                     scheduler_options={'dashboard_address': '8787'})\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"tcp://10.0.2.186:41037\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We extract features for the only non-ground points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details of the retiling schema\n",
    "grid = {\n",
    "    'min_x': -113107.81,\n",
    "    'max_x': 398892.19,\n",
    "    'min_y': 214783.87,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 512\n",
    "}\n",
    "\n",
    "# target mesh size\n",
    "tile_mesh_size = 10.\n",
    "\n",
    "# list of features\n",
    "z = \"normalized_height\" # most of the features are based on the normalized height\n",
    "features = [\"point_density\", \"sigma_z\"] \n",
    "features += [f\"perc_{percentile}_{z}\" for percentile in (25, 50, 75, 95)]\n",
    "features += [f\"{feature}_{z}\" for feature in (\"max\",\n",
    "                                              \"median\",\n",
    "                                              \"mean\", \n",
    "                                              \"var\", \n",
    "                                              \"std\", \n",
    "                                              \"skew\", \n",
    "                                              \"kurto\", \n",
    "                                              \"coeff_var\", \n",
    "                                              \"entropy\", \n",
    "                                              \"density_absolute_mean\")]\n",
    "#features += [f\"band_ratio_{z}<5\", f\"band_ratio_5<{z}<20\", f\"band_ratio_20<{z}\"]\n",
    "features += [f\"eigenv_{i}\" for i in range(1, 4)]\n",
    "features += [f\"normal_vector_{i}\" for i in range(1, 4)]\n",
    "\n",
    "# define custom feature extractors\n",
    "custom_feature_extractors = [{'extractor_name': 'BandRatioFeatureExtractor', \n",
    "                              'data_key': z, \n",
    "                              'lower_limit': low_lim,        \n",
    "                              'upper_limit': up_lim} for low_lim, up_lim in ((None, 5), (5, 20), (20, None))]\n",
    "\n",
    "# setup input dictionary to configure the feature extraction pipeline\n",
    "feature_extraction_input_non_ground = {\n",
    "    'setup_local_fs': {'input_folder': path_input.as_posix(),\n",
    "                       'output_folder': path_output.as_posix()},\n",
    "    'load': {'attributes': ['raw_classification', 'normalized_height']}, \n",
    "    'add_custom_features': {\n",
    "        'custom_feature_list': custom_feature_extractors\n",
    "    },\n",
    "    'apply_filter': {\n",
    "        'filter_type': 'select_equal',\n",
    "        'attribute': 'raw_classification',\n",
    "        #unclassified (1), ground (2), buildings (6), water (9), wire conductor (14), artificial objects (26), never classified (0)\n",
    "        'value': 1\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        'validate_precision': 1.e-3,  # solves numerical issues for 6 tiles which have points on the edge\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False,\n",
    "        'overwrite': True\n",
    "    },\n",
    "    'clear_cache' : {},\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('feature_extraction_non-ground.json', 'w') as f:\n",
    "    json.dump(feature_extraction_input_non_ground, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# extract the tile indices from the tile names\n",
    "tile_indices = [[int(el) for el in tile.name.split('.')[0].split('_')[1:]] for tile in tiles]\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [DataProcessing(t.name, tile_index=idx).config(feature_extraction_input_non_ground) \n",
    "               for t, idx in zip(tiles, tile_indices)]\n",
    "macro.set_labels([os.path.splitext(tile.name)[0] for tile in tiles])\n",
    "\n",
    "macro.setup_cluster(cluster=\"tcp://10.0.2.186:41037\")\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and write name of failed pipelines to file\n",
    "macro.print_outcome(to_file='feature_extraction_non-ground.out')\n",
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('feature_extraction_non-ground_failed.json', 'w') as f:\n",
    "        json.dump(['.'.join([pip.label, 'laz']) for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: 'tcp://145.100.59.123:8786' processes=20 threads=20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 907, in _run\n",
      "    return self.callback()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/client.py\", line 1157, in _heartbeat\n",
      "    self.scheduler_comm.send({\"op\": \"heartbeat-client\"})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/batched.py\", line 117, in send\n",
      "    raise CommClosedError\n",
      "distributed.comm.core.CommClosedError\n",
      "tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: 'tcp://145.100.59.123:8786' processes=20 threads=20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 907, in _run\n",
      "    return self.callback()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/client.py\", line 1157, in _heartbeat\n",
      "    self.scheduler_comm.send({\"op\": \"heartbeat-client\"})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/batched.py\", line 117, in send\n",
      "    raise CommClosedError\n",
      "distributed.comm.core.CommClosedError\n",
      "tornado.application - ERROR - Exception in callback <bound method Client._heartbeat of <Client: 'tcp://145.100.59.123:8786' processes=20 threads=20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tornado/ioloop.py\", line 907, in _run\n",
      "    return self.callback()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/client.py\", line 1157, in _heartbeat\n",
      "    self.scheduler_comm.send({\"op\": \"heartbeat-client\"})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/distributed/batched.py\", line 117, in send\n",
      "    raise CommClosedError\n",
      "distributed.comm.core.CommClosedError\n"
     ]
    }
   ],
   "source": [
    "macro.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel all jobs and restart the notebook\n",
    "\n",
    "Copy and paste these lines in a separate Python shell. If the Dask dashboard shows that some tasks are still queued to be processed, run the lines again - this should clear the scheduler up and give back control to the current notebook. Normally proceed to terminate the cluster and restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, Future\n",
    "client = Client('tcp://145.100.59.123:8786')\n",
    "futures = [Future(key) for key in client.who_has().keys()]\n",
    "client.cancel(futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
